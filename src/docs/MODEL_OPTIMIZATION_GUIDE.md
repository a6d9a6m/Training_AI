# æ¨¡å‹ä¼˜åŒ–æŒ‡å— - è§£å†³å‡é˜³æ€§ï¼ˆæ­£å¸¸æ ·æœ¬è¢«è¯¯åˆ¤ä¸ºå¼‚å¸¸ï¼‰

## é—®é¢˜è¯Šæ–­

å½“å‰æ¨¡å‹æŠŠ**æ­£å¸¸æ ·æœ¬è¯¯åˆ¤ä¸ºå¼‚å¸¸**ï¼ˆå‡é˜³æ€§ï¼‰ï¼Œè¯´æ˜æ¨¡å‹è¿‡äºæ•æ„Ÿï¼Œå¯¹æ­£å¸¸æ¨¡å¼çš„å­¦ä¹ ä¸å¤Ÿå……åˆ†ã€‚

## ä¼˜åŒ–æ–¹æ¡ˆï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰

### ğŸ”¥ æ–¹æ¡ˆ1ï¼šè°ƒæ•´æ¨¡å‹å‚æ•°ï¼ˆæœ€å¿«è§æ•ˆï¼‰

#### 1.1 é™ä½ IsolationForest çš„æ•æ„Ÿåº¦

åœ¨ `train_gmm_with_score_export.py` ç¬¬661è¡Œï¼š

```python
# å½“å‰ï¼ˆå¤ªä¸¥æ ¼ï¼‰
iso_forest = IsolationForest(n_estimators=100, contamination=0.1, random_state=42, n_jobs=-1)

# ä¼˜åŒ–å
iso_forest = IsolationForest(
    n_estimators=200,           # å¢åŠ æ ‘çš„æ•°é‡ï¼Œæé«˜ç¨³å®šæ€§
    contamination=0.05,         # é™ä½åˆ°5%ï¼Œå‡å°‘è¯¯æŠ¥
    max_samples=256,            # é™åˆ¶æ¯æ£µæ ‘çš„æ ·æœ¬æ•°
    random_state=42,
    n_jobs=-1
)
```

#### 1.2 è°ƒæ•´ OneClassSVM å‚æ•°

åœ¨ `train_gmm_with_score_export.py` ç¬¬665è¡Œï¼š

```python
# å½“å‰ï¼ˆè¾¹ç•Œå¤ªç´§ï¼‰
ocsvm = OneClassSVM(kernel='rbf', gamma='auto', nu=0.1)

# ä¼˜åŒ–å
ocsvm = OneClassSVM(
    kernel='rbf',
    gamma='scale',              # ä½¿ç”¨ scale ä»£æ›¿ autoï¼Œæ›´ç¨³å®š
    nu=0.05,                    # é™ä½åˆ°5%ï¼Œæ”¾å®½è¾¹ç•Œ
    tol=1e-4                    # æé«˜å®¹å¿åº¦
)
```

#### 1.3 å¢åŠ  GMM ç»„ä»¶æ•°

åœ¨ `train_gmm_with_score_export.py` ç¬¬652/672è¡Œï¼š

```python
# å½“å‰
best_n, best_cov = find_best_gmm_params(train_features_selected, val_features_selected, max_components=8)

# ä¼˜åŒ–å
best_n, best_cov = find_best_gmm_params(train_features_selected, val_features_selected, max_components=12)
```

æ›´å¤šGMMç»„ä»¶å¯ä»¥æ›´å¥½åœ°æ•æ‰æ­£å¸¸æ•°æ®çš„å¤šæ ·æ€§ã€‚

---

### ğŸ¯ æ–¹æ¡ˆ2ï¼šå¢å¼ºè®­ç»ƒæ•°æ®å¤šæ ·æ€§

#### 2.1 å¢åŠ æ•°æ®å¢å¼ºå¼ºåº¦

åœ¨ `train_gmm_with_score_export.py` ç¬¬48-75è¡Œï¼Œå¢åŠ æ›´å¤šå¢å¼ºæ–¹å¼ï¼š

```python
def augment_audio(audio, sr):
    """å¢å¼ºç‰ˆæ•°æ®å¢å¼º"""
    augmented = [audio]

    # 1. æ—¶é—´æ‹‰ä¼¸ï¼ˆæ›´å¤šå˜åŒ–ï¼‰
    for rate in [0.85, 0.9, 0.95, 1.05, 1.1, 1.15]:
        try:
            audio_stretched = librosa.effects.time_stretch(audio, rate=rate)
            augmented.append(audio_stretched)
        except:
            pass

    # 2. éŸ³è°ƒåç§»
    for n_steps in [-3, -2, -1, 1, 2, 3]:
        try:
            audio_shifted = librosa.effects.pitch_shift(audio, sr=sr, n_steps=n_steps)
            augmented.append(audio_shifted)
        except:
            pass

    # 3. æ·»åŠ å™ªå£°ï¼ˆå¤šä¸ªå¼ºåº¦ï¼‰
    for noise_level in [0.003, 0.005, 0.008]:
        try:
            noise = np.random.normal(0, noise_level, len(audio))
            audio_noisy = audio + noise
            augmented.append(audio_noisy)
        except:
            pass

    # 4. éŸ³é‡å˜åŒ–
    for gain in [0.8, 0.9, 1.1, 1.2]:
        try:
            audio_gain = audio * gain
            augmented.append(audio_gain)
        except:
            pass

    return augmented
```

#### 2.2 ä½¿ç”¨æ›´å¤šè®­ç»ƒæ•°æ®

åœ¨ `train_gmm_with_score_export.py` ç¬¬505è¡Œï¼š

```python
# å½“å‰ï¼ˆ80%è®­ç»ƒï¼Œ20%æµ‹è¯•ï¼‰
train_val, normal_test = train_test_split(normal_data, test_size=0.2, random_state=42)

# ä¼˜åŒ–åï¼ˆ90%è®­ç»ƒï¼Œ10%æµ‹è¯•ï¼‰
train_val, normal_test = train_test_split(normal_data, test_size=0.1, random_state=42)
```

---

### ğŸ§ª æ–¹æ¡ˆ3ï¼šè°ƒæ•´é˜ˆå€¼ç­–ç•¥

#### 3.1 ä½¿ç”¨æ›´ä¿å®ˆçš„é˜ˆå€¼

åœ¨è®­ç»ƒå®Œæˆåï¼Œæ‰‹åŠ¨è°ƒæ•´é˜ˆå€¼ã€‚æŸ¥çœ‹ `sample_scores.csv` ä¸­çš„åˆ†æ•°åˆ†å¸ƒï¼š

```python
import pandas as pd
import numpy as np

# è¯»å–åˆ†æ•°
df = pd.read_csv('models/saved_models/sample_scores.csv')

# åˆ†ææ­£å¸¸æ ·æœ¬çš„åˆ†æ•°åˆ†å¸ƒ
normal_scores = df[df['label'] == 0]['gmm_score']

# ä½¿ç”¨æ›´é«˜çš„ç™¾åˆ†ä½æ•°ä½œä¸ºé˜ˆå€¼ï¼ˆå‡å°‘è¯¯æŠ¥ï¼‰
threshold_95 = np.percentile(normal_scores, 95)  # å½“å‰å¯èƒ½ç”¨90%
threshold_98 = np.percentile(normal_scores, 98)  # æ›´ä¿å®ˆ
threshold_99 = np.percentile(normal_scores, 99)  # éå¸¸ä¿å®ˆ

print(f"å»ºè®®é˜ˆå€¼èŒƒå›´: {threshold_95:.2f} - {threshold_99:.2f}")
```

#### 3.2 é›†æˆæ¨¡å‹ä½¿ç”¨è½¯æŠ•ç¥¨

ä¿®æ”¹ `train_ensemble_from_scores.py` ç¬¬50è¡Œï¼Œä½¿ç”¨æ¦‚ç‡é˜ˆå€¼ï¼š

```python
# å½“å‰ï¼ˆç¡¬åˆ¤æ–­ï¼‰
rf = RandomForestClassifier(
    n_estimators=100,
    max_depth=5,
    random_state=42,
    n_jobs=-1
)

# ä¼˜åŒ–åï¼ˆå¢åŠ æ·±åº¦ï¼Œå¢åŠ æ ‘çš„æ•°é‡ï¼‰
rf = RandomForestClassifier(
    n_estimators=300,           # æ›´å¤šæ ‘
    max_depth=10,               # æ›´æ·±çš„æ ‘
    min_samples_split=10,       # éœ€è¦æ›´å¤šæ ·æœ¬æ‰åˆ†è£‚
    min_samples_leaf=5,         # å¶èŠ‚ç‚¹æœ€å°‘æ ·æœ¬æ•°
    class_weight={0: 1, 1: 2},  # å¢åŠ å¼‚å¸¸ç±»çš„æƒé‡ï¼ˆå¦‚æœå¼‚å¸¸å¤ªå°‘ï¼‰
    random_state=42,
    n_jobs=-1
)
```

ç„¶ååœ¨é¢„æµ‹æ—¶ä½¿ç”¨æ¦‚ç‡é˜ˆå€¼ï¼š

```python
# åœ¨ predict_with_ensemble.py ä¸­
proba = model.predict_proba(X)[0]

# å½“å‰ï¼šç›´æ¥ä½¿ç”¨ argmax
prediction = model.predict(X)[0]  # 0.5é˜ˆå€¼

# ä¼˜åŒ–åï¼šä½¿ç”¨æ›´é«˜çš„é˜ˆå€¼åˆ¤å®šä¸ºå¼‚å¸¸
ANOMALY_THRESHOLD = 0.7  # å¼‚å¸¸æ¦‚ç‡è¦>70%æ‰åˆ¤ä¸ºå¼‚å¸¸
prediction = 1 if proba[1] > ANOMALY_THRESHOLD else 0
```

---

### ğŸ”§ æ–¹æ¡ˆ4ï¼šç‰¹å¾å·¥ç¨‹ä¼˜åŒ–

#### 4.1 å¢åŠ ç‰¹å¾é²æ£’æ€§

åœ¨ `train_gmm_with_score_export.py` ç¬¬576-580è¡Œï¼Œä½¿ç”¨æ›´é²æ£’çš„ç¼©æ”¾å™¨ï¼š

```python
# å½“å‰
scaler = RobustScaler()

# ä¼˜åŒ–åï¼ˆå¤šç§ç¼©æ”¾å™¨æµ‹è¯•ï¼‰
from sklearn.preprocessing import RobustScaler, StandardScaler, PowerTransformer

# æ–¹æ¡ˆAï¼šä½¿ç”¨PowerTransformerå¤„ç†åæ€åˆ†å¸ƒ
scaler = PowerTransformer(method='yeo-johnson', standardize=True)

# æ–¹æ¡ˆBï¼šç»„åˆç¼©æ”¾å™¨
from sklearn.pipeline import Pipeline
scaler = Pipeline([
    ('robust', RobustScaler()),
    ('power', PowerTransformer(method='yeo-johnson'))
])
```

#### 4.2 å‡å°‘è¿‡æ‹Ÿåˆçš„ç‰¹å¾é€‰æ‹©

åœ¨ `train_gmm_with_score_export.py` ç¬¬588è¡Œï¼š

```python
# å½“å‰
k_features = min(args.k_features, train_features_scaled.shape[1])  # é»˜è®¤60

# ä¼˜åŒ–åï¼ˆé€‰æ‹©æ›´å°‘ä½†æ›´ç¨³å®šçš„ç‰¹å¾ï¼‰
k_features = min(40, train_features_scaled.shape[1])  # å‡å°‘åˆ°40

# æˆ–è€…ä½¿ç”¨æ–¹å·®é˜ˆå€¼é¢„ç­›é€‰
from sklearn.feature_selection import VarianceThreshold
var_selector = VarianceThreshold(threshold=0.1)  # å»é™¤ä½æ–¹å·®ç‰¹å¾
train_features_scaled = var_selector.fit_transform(train_features_scaled)
```

---

### ğŸ“Š æ–¹æ¡ˆ5ï¼šæ¨¡å‹é›†æˆç­–ç•¥ä¼˜åŒ–

#### 5.1 åŠ æƒé›†æˆï¼ˆè€Œéç®€å•æŠ•ç¥¨ï¼‰

åˆ›å»ºæ–°æ–‡ä»¶ `train_weighted_ensemble.py`ï¼š

```python
def train_weighted_ensemble(df, score_cols):
    """ä½¿ç”¨åŠ æƒå¹³å‡ï¼Œç»™å‡é˜³æ€§é«˜çš„æ¨¡å‹é™ä½æƒé‡"""
    X = df[score_cols].values
    y = df['label'].values

    # åˆ†ææ¯ä¸ªæ¨¡å‹çš„å‡é˜³æ€§ç‡
    weights = {}
    for col in score_cols:
        scores = df[col].values

        # æ‰¾æœ€ä¼˜é˜ˆå€¼
        thresholds = np.linspace(scores.min(), scores.max(), 100)
        best_fpr = 1.0

        for thresh in thresholds:
            preds = (scores > thresh).astype(int)
            tn, fp, fn, tp = confusion_matrix(y, preds).ravel()
            fpr = fp / (fp + tn)  # å‡é˜³æ€§ç‡

            if fpr < best_fpr:
                best_fpr = fpr

        # å‡é˜³æ€§ç‡è¶Šä½ï¼Œæƒé‡è¶Šé«˜
        weights[col] = 1.0 / (best_fpr + 0.01)

    # å½’ä¸€åŒ–æƒé‡
    total_weight = sum(weights.values())
    weights = {k: v/total_weight for k, v in weights.items()}

    print("æ¨¡å‹æƒé‡ï¼ˆåŸºäºå‡é˜³æ€§ç‡ï¼‰:")
    for col, w in weights.items():
        print(f"  {col}: {w:.3f}")

    return weights
```

---

## å¿«é€Ÿæµ‹è¯•æ–¹æ¡ˆ

### Step 1: å…ˆè°ƒæ•´å‚æ•°å†é‡æ–°è®­ç»ƒ

```bash
# ä¿®æ”¹ train_gmm_with_score_export.py ä¸­çš„å‚æ•°åé‡æ–°è®­ç»ƒ
python src/train/train_gmm_with_score_export.py `
    --normal_train_dir data/normal `
    --anomaly_test_dir data/anomaly `
    --use_deep_features `
    --use_ensemble `
    --use_augmentation `
    --output_dir models/saved_models_optimized
```

### Step 2: é‡æ–°è®­ç»ƒé›†æˆæ¨¡å‹

```bash
python src/train/train_ensemble_from_scores.py --scores_csv models/saved_models_optimized/sample_scores.csv --output_dir models/saved_models_optimized
```

### Step 3: æµ‹è¯•å¹¶å¯¹æ¯”

```bash
# æµ‹è¯•æ–°æ¨¡å‹
python predict_with_ensemble.py \
    --base_model models/saved_models_optimized/gmm_with_scores.pkl \
    --ensemble_model models/saved_models_optimized/ensemble_model.pkl \
    --audio_dir dev_data/fan/train

# å¯¹æ¯”æ—§æ¨¡å‹
python predict_with_ensemble.py \
    --base_model models/saved_models/gmm_with_scores.pkl \
    --ensemble_model models/saved_models/ensemble_model.pkl \
    --audio_dir dev_data/fan/train
```

---

## è¯Šæ–­å·¥å…·

åˆ›å»ºè„šæœ¬ `diagnose_false_positives.py` æ¥åˆ†æè¯¯åˆ¤æ ·æœ¬ï¼š

```python
import pandas as pd
import numpy as np

# è¯»å–é¢„æµ‹ç»“æœ
df = pd.read_csv('dev_data/prediction_results.csv')

# ç­›é€‰å‡é˜³æ€§æ ·æœ¬
false_positives = df[(df['true_label'] == 0) & (df['prediction'] == 1)]

print(f"å‡é˜³æ€§æ ·æœ¬æ•°: {len(false_positives)}")
print(f"å‡é˜³æ€§ç‡: {len(false_positives) / len(df[df['true_label']==0]):.2%}")

# åˆ†æåˆ†æ•°åˆ†å¸ƒ
print("\nå‡é˜³æ€§æ ·æœ¬çš„åˆ†æ•°ç‰¹å¾:")
for col in ['gmm_score', 'iso_score', 'ocsvm_score']:
    if col in false_positives.columns:
        print(f"  {col}: å‡å€¼={false_positives[col].mean():.2f}, "
              f"æ ‡å‡†å·®={false_positives[col].std():.2f}")

# å¯¹æ¯”æ­£ç¡®åˆ†ç±»çš„æ­£å¸¸æ ·æœ¬
true_negatives = df[(df['true_label'] == 0) & (df['prediction'] == 0)]
print("\næ­£ç¡®åˆ†ç±»çš„æ­£å¸¸æ ·æœ¬çš„åˆ†æ•°ç‰¹å¾:")
for col in ['gmm_score', 'iso_score', 'ocsvm_score']:
    if col in true_negatives.columns:
        print(f"  {col}: å‡å€¼={true_negatives[col].mean():.2f}, "
              f"æ ‡å‡†å·®={true_negatives[col].std():.2f}")
```

---

## æ¨èä¼˜åŒ–é¡ºåº

1. **ç«‹å³æ‰§è¡Œ**ï¼ˆ30åˆ†é’Ÿå†…ï¼‰ï¼š
   - è°ƒæ•´ IsolationForest `contamination=0.05`
   - è°ƒæ•´ OneClassSVM `nu=0.05`
   - é‡æ–°è®­ç»ƒå¹¶æµ‹è¯•

2. **çŸ­æœŸä¼˜åŒ–**ï¼ˆ1-2å°æ—¶ï¼‰ï¼š
   - å¢å¼ºæ•°æ®å¢å¼ºä»£ç 
   - å¢åŠ è®­ç»ƒæ•°æ®æ¯”ä¾‹åˆ°90%
   - è°ƒæ•´é›†æˆæ¨¡å‹å‚æ•°

3. **ä¸­æœŸä¼˜åŒ–**ï¼ˆåŠå¤©ï¼‰ï¼š
   - å®ç°åŠ æƒé›†æˆ
   - ä¼˜åŒ–ç‰¹å¾é€‰æ‹©
   - äº¤å‰éªŒè¯é˜ˆå€¼

4. **é•¿æœŸä¼˜åŒ–**ï¼ˆ1-2å¤©ï¼‰ï¼š
   - æ”¶é›†æ›´å¤šæ­£å¸¸æ ·æœ¬
   - å°è¯•æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼ˆVAEã€Transformerï¼‰
   - å®ç°åœ¨çº¿å­¦ä¹ /æŒç»­å­¦ä¹ 

---

## å…³é”®æŒ‡æ ‡ç›‘æ§

è®­ç»ƒåéœ€è¦å…³æ³¨ï¼š

- **å‡é˜³æ€§ç‡ï¼ˆFPRï¼‰**ï¼šç›®æ ‡ < 5%
- **å¬å›ç‡ï¼ˆRecallï¼‰**ï¼šç›®æ ‡ > 85%ï¼ˆä¸èƒ½ä¸ºäº†é™ä½è¯¯æŠ¥è€Œæ¼æ‰çœŸå¼‚å¸¸ï¼‰
- **F1åˆ†æ•°**ï¼šç»¼åˆæŒ‡æ ‡ï¼Œç›®æ ‡ > 0.85
- **Cohen's d**ï¼šåˆ†ç¦»åº¦æŒ‡æ ‡ï¼Œç›®æ ‡ > 0.8

è¿è¡Œ `sample_scores.csv` åˆ†ææ—¶æŸ¥çœ‹ï¼š
```
å¯åˆ†ç¦»æ€§æŒ‡æ ‡ï¼ˆCohen's dï¼‰: X.XXXX
  > 0.8: å¤§æ•ˆåº”ï¼ˆæ˜“åˆ†ç¦»ï¼‰  â† ç›®æ ‡
  0.5-0.8: ä¸­ç­‰æ•ˆåº”
  < 0.5: éš¾åˆ†ç¦»  â† å½“å‰å¯èƒ½åœ¨è¿™é‡Œ
```
